<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My First Website</title>
    <link>/</link>
    <description>Recent content on My First Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Nov 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Resampling Method</title>
      <link>/2018/11/01/resampling-method/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/01/resampling-method/</guid>
      <description>重抽样方法 1.验证集方法(validation set approach) 1.随机的把观测集分成两部分:一个训练集(training set)和一个验证集(validation set)[保留集(hold-out set)].
模型在训练集上拟合,然后用拟合的模型来预测验证集中观测的响应变量,得到的验证集错误率(通常用均方误差)作为定量响应变量的误差度量,提供了对测试错误率的一个估计.
2.缺陷:
测试错误率因为验证集的选取不同波动很大
只有训练集中的观测被用于拟合模型,验证集错误率会高估在整个数据集上拟合模型所得到的测试错误率.
 2.留一交叉验证法(leave-one-out cross-validation, LOOCV) 1.将单独的一个观测作为验证集\((x_1,y_1)\),剩下的观测组成训练集拟合模型,再用\((x_1,y_1)\)验证,对每个观测重复这一步骤,得到n个MSE:\[CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}{MSE_i}\] \(CV_{(n)}\)是测试均方误差的无偏估计.
2.优点:1.偏差较小,不容易高估测试误差率.2.LOOCV方法不存在随机性 缺点:计算量大. 3.用最小二乘法拟合线性回归或多项式回归时,可简化为:\[CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}{(\frac{y_i-\hat{y_i}}{1-h_i})^{2}}\] \(h_i\)是杠杆值:\[h_i=\frac{1}{n}+\frac{(x_i-\overline{x})^{2}}{\sum_{i^{&amp;#39;}=1}^{n}{(x_{i^{&amp;#39;}}-\overline{x})^{2}}}\] \(h_i\)的取值在\(\frac{1}{n}\)和1之间,若远远超过\(\frac{p+1}{n}\),则对应点有较高的杠杆作用.
 3.k折交叉验证法(k-fold CV) 1.将观测分为k个大小基本一致的组(或者说折fold),第一折作为验证集,然后在剩下的k-1折拟合模型,重复这个步骤k次,每一次把不同的观测组作为验证集,会得到k个测试误差的估计(MSE),k折CV估计用这些值求平均得到:\[CV_{(k)}=\frac{1}{k}\sum_{i=1}^{k}{MSE_i}\] 一般令k=5或k=10.
2.优点:1.计算量小.2.对测试误差的估计通常更准确.
偏差-方差权衡问题:LOOCV的每一个模型高度相关,k折CV的方法相对的相关性低,高度相关的量的均值要比相关性较小的量的均值具有更高的波动性,因此LOOCV对测试均方误差的估计虽然偏差更小但方差更大.
 4.分类问题 分类问题用错误率替代均方误差:\[CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}{Err_i}\] 其中\(Err_i=I(y_i≠\hat{y_i})\).
 5.自助法(bootstrap) 自助法通过反复从原始数据集中抽取观测得到数据集:抽样通过有放回的形式进行,抽样得到\(Z^{*1}\),每次抽样结束后对参数进行估计,得到估计值\(\hat{\alpha^{*1}}\),这个步骤重复B次,其中B是一个很大的值,就可以产生B个数据集和相应的估计值,用这些估计值的均值来估计\(\hat{\alpha}\).
  </description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>/2018/10/31/liner-regression/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/31/liner-regression/</guid>
      <description>线性回归基本假设,影响及检验 1.数据满足线性关系 1.可通过查看残差图(residual plot)查看残差是否存在明显规律
2.若存在非线性关系,可对预测变量进行非线性转化\(logX\),\(X^{2}\),\(\sqrt{X}\)等
 2.误差项不相关 1.如果误差项之间有相关性(经常出现在时间序列),估计标准误往往低估了真实标准误.
2.因此,置信区间和预测区间比真实区间窄,95%置信区间包含真实参数的实际概率远低于95%.
3.可通过Durbin-Watson(DW)检测.
 3.误差项方差恒定 1.线性回归假设误差项方差是恒定的\(VAR(\epsilon_{i}) = \sigma^{2}\).
2.同样通过看残差图确定方差是否恒定,如果残差图呈漏斗形,说明方差非恒定(存在异方差性).
3.若存在异方差性,用凹函数对响应值Y做变换:\(logY\),\(\sqrt{Y}\)等.
 4.自变量之间相互独立 1.若自变量之间存在多重共线性(Multicollinearity),会使很多系数估计都有同样的残差平方和(RSS),导致系数估计有很大的不确定性,降低了回归系数的准确性,导致\(\hat{\beta_{j}}\)的标准误变大(t统计量由\(\hat{\beta_{j}}\)除以它的标准误得到),假设检验的效力减小了.
2.检测共线性的简单方法是看预测变量的相关系数矩阵,但并非所有共线性问题都可以通过检查相关系数矩阵检测到.
3.更好的评估多重共线性的方法是计算方差膨胀因子(variance inflation factor, VIF).\[VIF(\hat{\beta_{j}}) = \frac{1}{1-R^{2}_{X_{j}|X_{-j}}}\] \(R^{2}_{X_{j}|X_{-j}}\)是\(X_{j}\)对所有预测变量回归的\(R^{2}\).
4.若VIF&amp;lt;3说明该变量基本不存在多重共线性问题,若VIF&amp;gt;10说明多重共线性问题比较严重. R中可用car包中vif()函数查看方差膨胀因子.
 5.误差项呈正态分布 1.如果误差项不呈正态分布,意味着置信区间会变得很不稳定,我们往往需要重点关注一些异常的点(误差较大但出现频率较高),来得到更好的模型.
2.通过查看Q-Q图来检测,如果图中散点呈直线表示误差项呈正态分布,否则非正态分布.
  高杠杆点与离群点 1.离群点(outlier):指\(y_i\)远离模型预测值的点
2.高杠杆点(high leverage):表示观测点\(x_i\)是异常的
 </description>
    </item>
    
    <item>
      <title>Artificial Neural Network</title>
      <link>/2018/10/29/ann/</link>
      <pubDate>Mon, 29 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/29/ann/</guid>
      <description>library(keras) # classic mnist dataset mnist &amp;lt;- dataset_mnist() # training set train_images &amp;lt;- mnist$train$x train_labels &amp;lt;- mnist$train$y # test set test_images &amp;lt;- mnist$test$x test_labels &amp;lt;- mnist$test$y rescale so that values are in the [0, 1] interval train_images &amp;lt;- array_reshape(train_images, c(60000, 28 * 28)) train_images &amp;lt;- train_images / 255 test_images &amp;lt;- array_reshape(test_images, c(10000, 28 * 28)) test_images &amp;lt;- test_images / 255  network architecture network &amp;lt;- keras_model_sequential() %&amp;gt;% layer_dense(units = 512, activation = &amp;quot;relu&amp;quot;, input_shape = c(28 * 28)) %&amp;gt;% layer_dense(units = 10, activation = &amp;quot;softmax&amp;quot;)  complete our network network %&amp;gt;% compile( # how the network update itself # https://keras.</description>
    </item>
    
    <item>
      <title>My First Post</title>
      <link>/2018/10/23/my-first-post/</link>
      <pubDate>Tue, 23 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/23/my-first-post/</guid>
      <description>Ridge and Lasso Packages library(ISLR) library(tidyverse) library(glmnet)  sampling Hitters &amp;lt;- na.omit(Hitters) x &amp;lt;- model.matrix(Salary ~ ., Hitters)[, -1] y &amp;lt;- Hitters$Salary set.seed(1) train &amp;lt;- sample(1:nrow(x), nrow(x) / 2) test &amp;lt;- (-train) y.test &amp;lt;- y[test]  ridge regression grid &amp;lt;- 10 ^ seq(10, -2, length = 100) ridge.mod &amp;lt;- glmnet(x, y, alpha = 0, lambda = grid) plot(ridge.mod, main = &amp;quot;The ridge&amp;quot;)  the lasso lasso.mod &amp;lt;- glmnet(x[train, ], y[train], alpha = 1, lambda = grid) plot(lasso.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Thu, 05 May 2016 21:48:51 -0700</pubDate>
      
      <guid>/about/</guid>
      <description>This is a &amp;ldquo;hello world&amp;rdquo; example website for the blogdown package. The theme was forked from @jrutheiser/hugo-lithium-theme and modified by Yihui Xie.</description>
    </item>
    
  </channel>
</rss>